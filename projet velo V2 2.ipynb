{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import pymysql\n",
    "import mysql.connector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE LILLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# URL de l'API\n",
    "url = \"https://data.lillemetropole.fr/data/ogcapi/collections/ilevia:vlille_temps_reel/items?f=json&limit=-1\"\n",
    "\n",
    "# Faire une requête GET à l'API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier si la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    # Vérifier la structure de la réponse JSON\n",
    "    if 'records' in data:\n",
    "        records = data['records']\n",
    "        \n",
    "        station_v_lille = []\n",
    "        # Boucle pour extraire les records\n",
    "        for record in records:\n",
    "            station = {\n",
    "                'id': record['@id'],\n",
    "                'nom': record['nom'],\n",
    "                'adresse': record['adresse'],\n",
    "                'commune': record['commune'],\n",
    "                'etat': record['etat'],\n",
    "                'type': record['type'],\n",
    "                'nb_places_dispo': record['nb_places_dispo'],\n",
    "                'nb_velos_dispo': record['nb_velos_dispo'],\n",
    "                'etat_connexion': record['etat_connexion'],\n",
    "                'x': record['x'],\n",
    "                'y': record['y'],\n",
    "                'date_modification': record['date_modification']\n",
    "            }\n",
    "            station_v_lille.append(station)\n",
    "\n",
    "        # Convertir la liste de dictionnaires en DataFrame pandas\n",
    "        df_stations_lille = pd.DataFrame(station_v_lille)\n",
    "        \n",
    "        # Afficher le DataFrame sous forme de tableau\n",
    "        display(df_stations_lille)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE PARIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de l'API\n",
    "url = \"https://opendata.paris.fr/api/explore/v2.1/catalog/datasets/velib-disponibilite-en-temps-reel/records\"\n",
    "\n",
    "# Fonction pour collecter toutes les données de l'API avec pagination\n",
    "def collect_velib_data(url):\n",
    "    velib_data = []\n",
    "    start = 0\n",
    "    rows_per_page = 100  # Ajuster le nombre de lignes par page si nécessaire\n",
    "\n",
    "    while True:\n",
    "        # Faire une requête API avec les paramètres de pagination\n",
    "        response = requests.get(url, params={\"start\": start, \"rows\": rows_per_page})\n",
    "        \n",
    "        # Vérifier si la requête a réussi\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()\n",
    "                \n",
    "                # Vérifier si la clé \"results\" est dans la réponse\n",
    "                if \"results\" in data:\n",
    "                    # Boucler à travers chaque enregistrement de station de vélo dans \"results\"\n",
    "                    for record in data[\"results\"]:\n",
    "                        # Extraire les champs pertinents\n",
    "                        station_info = {\n",
    "                            \"stationcode\": record.get(\"stationcode\"),\n",
    "                            \"name\": record.get(\"name\"),\n",
    "                            \"is_installed\": record.get(\"is_installed\"),\n",
    "                            \"capacity\": record.get(\"capacity\"),\n",
    "                            \"numdocksavailable\": record.get(\"numdocksavailable\"),\n",
    "                            \"numbikesavailable\": record.get(\"numbikesavailable\"),\n",
    "                            \"mechanical_bikes\": record.get(\"mechanical\"),\n",
    "                            \"electric_bikes\": record.get(\"ebike\"),\n",
    "                            \"is_renting\": record.get(\"is_renting\"),\n",
    "                            \"is_returning\": record.get(\"is_returning\"),\n",
    "                            \"duedate\": record.get(\"duedate\"),\n",
    "                            \"latitude\": record[\"coordonnees_geo\"][\"lat\"],\n",
    "                            \"longitude\": record[\"coordonnees_geo\"][\"lon\"],\n",
    "                            \"nom_arrondissement_communes\": record.get(\"nom_arrondissement_communes\"),\n",
    "                            \"code_insee_commune\": record.get(\"code_insee_commune\")\n",
    "                        }\n",
    "                        # Ajouter l'enregistrement à la liste\n",
    "                        velib_data.append(station_info)\n",
    "                    \n",
    "                    # Si moins d'enregistrements que la limite demandée, nous avons atteint la fin\n",
    "                    if len(data[\"results\"]) < rows_per_page:\n",
    "                        break\n",
    "                    else:\n",
    "                        start += rows_per_page  # Passer à la page suivante\n",
    "                else:\n",
    "                    print(\"Clé 'results' non trouvée dans la réponse JSON.\")\n",
    "                    break\n",
    "            except ValueError:\n",
    "                print(\"Erreur de décodage JSON.\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"La requête a échoué avec le code de statut {response.status_code}.\")\n",
    "            break\n",
    "\n",
    "    return velib_data\n",
    "\n",
    "# Collecter les données\n",
    "velib_data = collect_velib_data(url)\n",
    "\n",
    "# Convertir la liste de dictionnaires en DataFrame\n",
    "df_stations_paris = pd.DataFrame(velib_data)\n",
    "\n",
    "# Afficher le DataFrame sous forme de tableau\n",
    "display(df_stations_paris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE TOULOUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de l'API\n",
    "url = \"https://data.toulouse-metropole.fr/api/explore/v2.1/catalog/datasets/stationnement-velo/records?\"\n",
    "\n",
    "def collect_velib_data(url):\n",
    "    velib_data_toulouse = []\n",
    "    start = 0\n",
    "    rows_per_page = 100\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, params={\"start\": start, \"rows\": rows_per_page})\n",
    "            data = response.json()\n",
    "\n",
    "            if \"results\" in data:\n",
    "                for record in data[\"results\"]:\n",
    "                    station_info = {\n",
    "                        \"lib_voie\": record.get(\"lib_voie\"),\n",
    "                        \"motdir\": record.get(\"motdir\"),\n",
    "                        \"no\": record.get(\"no\"),\n",
    "                        \"commune\": record.get(\"commune\"),\n",
    "                        \"insee\": record.get(\"insee\"),\n",
    "                        \"nb_places\": record.get(\"nb_places\"),\n",
    "                        \"coord_x\": record.get(\"coord_x\"),\n",
    "                        \"coord_y\": record.get(\"coord_y\"),\n",
    "                        \"latitude\": record.get(\"geo_point_2d\", {}).get(\"lat\"),\n",
    "                        \"longitude\": record.get(\"geo_point_2d\", {}).get(\"lon\")\n",
    "                    }\n",
    "                    velib_data_toulouse.append(station_info)\n",
    "\n",
    "                # Vérifier s'il y a plus d'enregistrements à récupérer\n",
    "                if len(data[\"results\"]) < rows_per_page:\n",
    "                    break\n",
    "                start += rows_per_page\n",
    "            else:\n",
    "                break\n",
    "        except ValueError:\n",
    "            break\n",
    "    return velib_data_toulouse\n",
    "\n",
    "# Collecter les données\n",
    "velib_data_toulouse = collect_velib_data(url)\n",
    "print(velib_data_toulouse)\n",
    "\n",
    "# Convertir la liste de dictionnaires en DataFrame\n",
    "df_stations_toulouse = pd.DataFrame(velib_data_toulouse)\n",
    "\n",
    "# Afficher le DataFrame sous forme de tableau\n",
    "display(df_stations_toulouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trier et nettoyer les bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lille "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_lille.head () "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage et transformation des données en une seule cellule\n",
    "df_stations_lille = df_stations_lille.drop(columns=['id', 'etat', 'type', 'etat_connexion', 'date_modification', 'adresse'])\n",
    "df_stations_lille = df_stations_lille.rename(columns={'x': 'latitude', 'y': 'longitude', 'nom': 'nom_station'})\n",
    "\n",
    "# Affichage des premières lignes du DataFrame nettoyé\n",
    "df_stations_lille.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_paris.head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage et transformation des données en une seule cellule\n",
    "df_stations_paris = df_stations_paris.drop(columns=[\n",
    "    'numbikesavailable', 'mechanical_bikes', 'electric_bikes', \n",
    "    'is_renting', 'is_returning', 'duedate', 'stationcode', \n",
    "    'is_installed', 'code_post'\n",
    "])\n",
    "df_stations_paris = df_stations_paris.rename(columns={\n",
    "    'name': 'nom_station',\n",
    "    'nom_arrondissement_communes': 'commune',\n",
    "    'code_insee_commune': 'code_post',\n",
    "    'numdocksavailable': 'nb_velos_dispo',\n",
    "    'capacity': 'nb_places_dispo'\n",
    "})\n",
    "\n",
    "# Modification de la colonne 'nom_station'\n",
    "df_stations_paris['nom_station'] = df_stations_paris['nom_station'].str.split('-').str[0]\n",
    "\n",
    "# Affichage des premières lignes du DataFrame nettoyé\n",
    "df_stations_paris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_toulouse.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toulouse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_toulouse.head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage et transformation des données en une seule cellule\n",
    "df_stations_toulouse = df_stations_toulouse.rename(columns={\n",
    "    'lib_voie': 'nom_station',\n",
    "    'no': 'nb_velos_dispo',\n",
    "    'insee': 'code_post',\n",
    "    'nb_places': 'nb_places_dispo'\n",
    "})\n",
    "df_stations_toulouse = df_stations_toulouse.drop(columns=[\n",
    "    'coord_x', 'coord_y', 'motdir', 'code_post'\n",
    "])\n",
    "\n",
    "# Affichage des premières lignes du DataFrame nettoyé\n",
    "df_stations_toulouse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df_stations_toulouse.isnull().sum()\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_toulouse = df_stations_toulouse.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_toulouse.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliser la colonne nom_station dans df_stations_toulouse\n",
    "df_stations_toulouse['nom_station'] = df_stations_toulouse['nom_station'].str.capitalize()\n",
    "\n",
    "# Normaliser la colonne nom_station dans df_stations_lille\n",
    "df_stations_lille['nom_station'] = df_stations_lille['nom_station'].str.capitalize()\n",
    "\n",
    "# Normaliser la colonne nom_station dans df_stations_paris\n",
    "df_stations_paris['nom_station'] = df_stations_paris['nom_station'].str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliser la colonne nom_station et commune dans df_stations_toulouse\n",
    "df_stations_toulouse['nom_station'] = df_stations_toulouse['nom_station'].str.capitalize()\n",
    "df_stations_toulouse['commune'] = df_stations_toulouse['commune'].str.capitalize()\n",
    "\n",
    "# Normaliser la colonne nom_station et commune dans df_stations_lille\n",
    "df_stations_lille['nom_station'] = df_stations_lille['nom_station'].str.capitalize()\n",
    "df_stations_lille['commune'] = df_stations_lille['commune'].str.capitalize()\n",
    "\n",
    "# Normaliser la colonne nom_station et commune dans df_stations_paris\n",
    "df_stations_paris['nom_station'] = df_stations_paris['nom_station'].str.capitalize()\n",
    "df_stations_paris['commune'] = df_stations_paris['commune'].str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_lille.head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_paris.head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_toulouse.head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténer les DataFrames\n",
    "df_final = pd.concat([df_stations_lille, df_stations_toulouse, df_stations_paris], ignore_index=True)\n",
    "\n",
    "# Afficher 20 lignes aléatoires du DataFrame final\n",
    "df_final.sample(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[(df_final['latitude'] != 0) & (df_final['longitude'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les valeurs non numériques par NaN\n",
    "df_final['nb_velos_dispo'] = pd.to_numeric(df_final['nb_velos_dispo'], errors='coerce')\n",
    "\n",
    "# Supprimer les lignes avec des NaN dans la colonne nb_velos_dispo\n",
    "df_final = df_final.dropna(subset=['nb_velos_dispo'])\n",
    "\n",
    "# Convertir la colonne nb_velos_dispo en int64\n",
    "df_final['nb_velos_dispo'] = df_final['nb_velos_dispo'].astype('int64')\n",
    "\n",
    "# Vérifier les types de données pour s'assurer du changement\n",
    "print(df_final.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecter le df à la base SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "user = 'root'\n",
    "password = 'root'\n",
    "host = 'localhost'\n",
    "unix_socket = '/Applications/MAMP/tmp/mysql/mysql.sock'\n",
    "database = 'velo'\n",
    "\n",
    "# Créez l'URL de connexion\n",
    "connection_string = f\"mysql+pymysql://{user}:{password}@{host}/{database}?unix_socket={unix_socket}\"\n",
    "\n",
    "# Créez l'objet engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "df_final = df_final\n",
    "\n",
    "try:\n",
    "    # Insérez le DataFrame dans la base de données\n",
    "    df_final.to_sql('velo', con=engine, if_exists='replace', index=False)\n",
    "    print(\"DataFrame inséré avec succès dans la base de données\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'insertion du DataFrame : {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
